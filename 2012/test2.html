<!DOCTYPE HTML>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>test2 | Exception&Life</title>
        <script>if (top !== self) top.location = self.location;</script>
        <meta name="HandheldFriendly" content="True" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=0" />
        
<meta name="author" content="shiweifu" />

        
        <link rel="alternate" type="application/rss+xml" href="/feed.xml" title="Exception&Life Feed" />
        <link rel="stylesheet" href="/static/style.css?v=986af" />
        
<link rel="canonical" href="/2012/test2.html" />
<link rel="stylesheet" href="/static/syntax.css?v=dbf77" />

        
     </head>
     <body>
         <header id="header">
             <nav class="fn-clear">
                 <ul id="nav" class="fn-right fn-clear">
                     <li id="nav-home"><a href="/">Home</a></li>
                     <li id="nav-tags"><a href="/tag/">Tags</a></li>
                 </ul>
             </nav>
         </header>
         
         <article class="hentry">
             <header class="entry-header fn-clear">
                 <h1 class="entry-title fn-left"><a href="/2012/test2.html">test2</a></h1>
                 
                 <form id="search-form" action="http://www.google.com/search">
                     <input type="hidden" name="q" value="site:http://shiweifu.github.com" />
                     <input id="search-input" class="ui-fm-text" type="text" placeholder="Search .." name="q" />
                 </form>
                 
             </header>
             
<div class="entry">
    <div class="entry-content">
        <h3>测试代码显示是否正常:</h3>
<pre><code>#! #usr#bin#python
#coding: utf-8

#author: shiweifu
#  mail: shiweifu@gmail.com

import urllib
import re
import BeautifulSoup
from urlparse import urlparse
from tidylib import tidy_document as tidy_html
import json
import time
import urllib2
import os
import random
import socket
import httplib
import logging
from ConfigParser import *
import bsddb
import itertools
from ua import get_random_ua

ini_file = ConfigParser
bs = BeautifulSoup.BeautifulSoup

#------------------------------global---------------------------#

tidy_page = lambda url: tidy_html(down_page(url))[0]

def down_page(url):
    global UA_LIST
    headers = { "User-Agent" : get_random_ua()}
    req = urllib2.Request(url, headers=headers)
    time.sleep(0.5)
    while True:
        try:
            response = urllib2.urlopen(req, timeout=10)
            the_page = response.read()
            break
        except:
            time.sleep(15)
            debug_log(exception="recv 503, tmd!!")
    return the_page

#------------------------------log------------------------------#

def get_format_log(**dic):
    """遍历dic，合并kv成为最终字符串
    &gt;&gt;&gt; print get_format_log(a="abc")
    a:abc
    """
    _log = ""
    for key in dic.keys():
        tmp = "".join((key, ":", dic[key]))
        _log = " ".join((_log, tmp))
    return _log.strip()

def log(logger, log_type):
    wrapper = lambda **kw: logger.log(log_type, get_format_log(**kw))
    return  wrapper

def initlog(fileName="logfile"):
    """ 初始化logger对象使用函数内的 fmt 作为格式
    &gt;&gt;&gt; import logging
    &gt;&gt;&gt; ret = initlog("abc")
    &gt;&gt;&gt; print isinstance(ret, logging.RootLogger)
    True
    """
    logger = logging.getLogger()
    hdlr = logging.FileHandler(fileName)
    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
    hdlr.setFormatter(formatter)
    logger.addHandler(hdlr)
    logger.setLevel(logging.NOTSET)
    return logger

logger = initlog("logfile")

debug_log = log(logger, logging.DEBUG)
warn_log = log(logger, logging.WARN)
error_log = log(logger, logging.ERROR)

#--------------------------operator-----------------------------#

def get_contain_items(page):
    regex = """href=\"(.*?)\"&gt;&lt;img\s.*?id=\"\w+[item|album]_d_(\d+)\""""
    try:
        catalog = str(bs(str(bs(page).body.findAll("div", {"class": "catalog"}))))
    except:
        debug_log(page=page)
        import sys
        sys.exit(-1)
    return re.findall(regex, catalog)

def get_all_number_pages(url):
    page = tidy_page(url)

    if url.find("search") &gt;= 0:
        pre = "p="
        url = url + "&amp;"
    else:
        pre = "?p="
    results = re.findall("p=\d*", page)

    if len(results) == 0:
        return [tidy_page(url + pre+str(1))]

    last = int(results[-2].split("=")[1])
    return [tidy_page(u) for u in [url + pre + str(page_n) for page_n in xrange(1, last + 1)]]

def get_album_imgs(url):
    """  """
    debug_log(url=url)
    urls = []
    results = []
    url = url.split("?")[0]
    album = urlparse(url).path
    number_pages = get_all_number_pages(url)
    [urls.extend(get_contain_items(page)) for page in number_pages]
    results.extend([list(x) + [album] for x in urls])
    debug_log(results=str(results))
    return results

def get_search_imgs(url):
    """  """
    urls = []
    results = []
    url = url.split("&amp;")[0]
    number_pages = get_all_number_pages(url)
    [urls.extend(get_contain_items(page)) for page in number_pages]
    debug_log(urls=str(urls))
    [results.extend(get_album_imgs(u[0])) for u in urls]
    return results

#---------------------------config------------------------------#

def parser_conf(fn="conf"):
    """ 解析配置文件 """
    conf = ini_file()
    conf.read(fn)
    try:
        save_path = conf.get("conf", "save_path")
    except NoOptionError:
        import os
        save_path = os.getcwd()

    try:
        db_path = conf.get("conf", "db_path")
    except NoOptionError:
        db_path = "img.db"

    try:
        size = conf.get("conf", "size").split(";")
    except NoOptionError:
        size = ["0x0"]

    try:
        urls = conf.get("conf", "urls").split(";")
    except NoOptionError:
        urls = ["http://www.topit.me/user/335565"]

    return {"save_path": save_path,
            "size": size,
            "urls": urls,
            "db_path": db_path}

def parser_db(fn):
    """ 读取数据库信息 """
    db = bsddb.btopen(fn)
    return db

def filter_size(func):
    def wrapper(**args):
        while(1):
            regex = """href=\"(.*)\"\sname=.*&gt;&lt;img\swidth=\"(\d+)\"\sheight=\"(\d+)\""""
            debug_log(args=str(args))
            img_url, width, height = re.findall(regex, args["page"])[0]

            if args.has_key("dst_size") == False:
                args = {"img_url": None}

            width = int(width)*1.25
            height = int(height)*1.25

            f_min = -args["floor"]
            f_max = args["floor"]

#            debug_log(dst_size=str(args["dst_size"]))

            for size in args["dst_size"]:
                w_gap = width-int(size[0])
                h_gap = height-int(size[1])

                if (w_gap &gt;= f_min and w_gap &lt;= f_max) and \
                   (h_gap &gt;= f_min and h_gap &lt;= f_max):
                   continue
                else:
                    args = {"img_url": None}
                    break
            args = {"width": width,
                    "height": height,
                    "img_url": img_url}
            break
        return func(**args)
    return wrapper

def filter_scale(func):
    def wrapper(**args):
        v = args["height"]/args["width"]
        if v &gt;=1.3 and v &lt;= 1.6:
            return func(**args)
        else:
            return func(img_url=None)
    return wrapper

@filter_size
@filter_scale
def get_img(**args):
    return args

def worker(tasks, db, save_path, dst_size):
    """ 判断大小,下载图片,写入数据库 """
    try:
        url, tid, url_path = tasks.pop()
    except IndexError:
        return

    tid = tid.encode("utf-8")
    if db.has_key(tid.encode("utf-8")) == True:
        return
    page = tidy_page(url)
    result = get_img(page=page, dst_size=dst_size, floor=20)
    if result["img_url"] == None:
        return
    img = down_page(result["img_url"])
    w = result["width"]
    h = result["height"]

    p = save_path+url_path
    try:
        os.listdir(p)
    except OSError:
        os.makedirs(p)
    file_path = "%s/%s%s" % (p, tid, ".jpg")
    open(file_path, "wb").write(img)
    v = "%s:%s:%s" % (url_path, w, h)
    debug_log(insert_db=v)
    db[tid] = v

#---------------------------start------------------------------#

def main():
    conf = parser_conf()
    db = parser_db(conf["db_path"])

    urls = []
    results = []

    #todo 加入判断数据库成员是否存在,如果不存在就干掉对应的key

    for url in conf["urls"]:
        if url.find("search") &gt;=0:
            results.extend(get_search_imgs(url))
        else:
            results.extend(get_album_imgs(url))
    # results = json.loads(open("blackberry.db", "r").read())

    while(len(results) &gt; 0):
        worker(results, db, conf["save_path"], conf["size"])

if __name__ == '__main__':
    main()
</code></pre>
    </div>
    <div class="fn-clear">
        <p class="entry-rel fn-right">
        
        
        <a id="next-entry" href="/2012/test.html" title="test " rel="next">Next»</a>
        
        </p>
    </div>
</div>
<div class="entry-meta fn-clear">
    <p class="entry-time fn-left">
    Created at: 
    <a href="/2012/" rel="index"><time class="updated" datetime="2012-07-14T00:00:00Z" pubdate>Sat, Jul 14, 2012</time></a>
    </p>
    
    <p class="entry-tags fn-left">
    Tagged in:
    
    <a href="/tag/life/" class="tag" rel="tag">life</a>
    
    </p>
    
</div>


         </article>
         
         <footer id="footer">
             <div class="copyright">Copyright by <a href="http://shiweifu.github.com">Exception&Life</a> | Powered by <a href="http://lepture.com/project/liquidluck">Felix Felicis</a></div>
             <p class="love">Lovely designed by <a href="http://lepture.com">lepture</a></p>
         </footer>
         <script type="text/javascript">
             var currentNav = window.currentNav || "nav-home";
             document.getElementById(currentNav).className = "current";
         </script>
     </body>
 </html>