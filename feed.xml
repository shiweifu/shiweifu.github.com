<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Exception&Life</title>
    <link href="http://shiweifu.github.com/feed.xml" rel="self" />
    
    <link href="http://shiweifu.github.com" />
    
    <updated>2012-07-14T11:56:50Z</updated>
    <id>http://shiweifu.github.com</id>
    <entry>
        <title type="html"><![CDATA[test2]]></title>
        <author><name>shiweifu</name><uri>http://shiweifu.github.com</uri></author>
        <link href="http://shiweifu.github.com/2012/test2.html"/>
        <updated>2012-07-14T00:00:00Z</updated>
        <published>2012-07-14T00:00:00Z</published>
        <id>http://shiweifu.github.com/2012/test2.html</id>
        <category scheme="http://shiweifu.github.com/tag/life/" term="life" label="Life" />
        <content type="html" xml:base="http://shiweifu.github.com" xml:lang="en">
            <![CDATA[ <h3>测试代码显示是否正常:</h3>
<pre><code>#! #usr#bin#python
#coding: utf-8

#author: shiweifu
#  mail: shiweifu@gmail.com

import urllib
import re
import BeautifulSoup
from urlparse import urlparse
from tidylib import tidy_document as tidy_html
import json
import time
import urllib2
import os
import random
import socket
import httplib
import logging
from ConfigParser import *
import bsddb
import itertools
from ua import get_random_ua

ini_file = ConfigParser
bs = BeautifulSoup.BeautifulSoup

#------------------------------global---------------------------#

tidy_page = lambda url: tidy_html(down_page(url))[0]

def down_page(url):
    global UA_LIST
    headers = { "User-Agent" : get_random_ua()}
    req = urllib2.Request(url, headers=headers)
    time.sleep(0.5)
    while True:
        try:
            response = urllib2.urlopen(req, timeout=10)
            the_page = response.read()
            break
        except:
            time.sleep(15)
            debug_log(exception="recv 503, tmd!!")
    return the_page

#------------------------------log------------------------------#

def get_format_log(**dic):
    """遍历dic，合并kv成为最终字符串
    &gt;&gt;&gt; print get_format_log(a="abc")
    a:abc
    """
    _log = ""
    for key in dic.keys():
        tmp = "".join((key, ":", dic[key]))
        _log = " ".join((_log, tmp))
    return _log.strip()

def log(logger, log_type):
    wrapper = lambda **kw: logger.log(log_type, get_format_log(**kw))
    return  wrapper

def initlog(fileName="logfile"):
    """ 初始化logger对象使用函数内的 fmt 作为格式
    &gt;&gt;&gt; import logging
    &gt;&gt;&gt; ret = initlog("abc")
    &gt;&gt;&gt; print isinstance(ret, logging.RootLogger)
    True
    """
    logger = logging.getLogger()
    hdlr = logging.FileHandler(fileName)
    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
    hdlr.setFormatter(formatter)
    logger.addHandler(hdlr)
    logger.setLevel(logging.NOTSET)
    return logger

logger = initlog("logfile")

debug_log = log(logger, logging.DEBUG)
warn_log = log(logger, logging.WARN)
error_log = log(logger, logging.ERROR)

#--------------------------operator-----------------------------#

def get_contain_items(page):
    regex = """href=\"(.*?)\"&gt;&lt;img\s.*?id=\"\w+[item|album]_d_(\d+)\""""
    try:
        catalog = str(bs(str(bs(page).body.findAll("div", {"class": "catalog"}))))
    except:
        debug_log(page=page)
        import sys
        sys.exit(-1)
    return re.findall(regex, catalog)

def get_all_number_pages(url):
    page = tidy_page(url)

    if url.find("search") &gt;= 0:
        pre = "p="
        url = url + "&amp;"
    else:
        pre = "?p="
    results = re.findall("p=\d*", page)

    if len(results) == 0:
        return [tidy_page(url + pre+str(1))]

    last = int(results[-2].split("=")[1])
    return [tidy_page(u) for u in [url + pre + str(page_n) for page_n in xrange(1, last + 1)]]

def get_album_imgs(url):
    """  """
    debug_log(url=url)
    urls = []
    results = []
    url = url.split("?")[0]
    album = urlparse(url).path
    number_pages = get_all_number_pages(url)
    [urls.extend(get_contain_items(page)) for page in number_pages]
    results.extend([list(x) + [album] for x in urls])
    debug_log(results=str(results))
    return results

def get_search_imgs(url):
    """  """
    urls = []
    results = []
    url = url.split("&amp;")[0]
    number_pages = get_all_number_pages(url)
    [urls.extend(get_contain_items(page)) for page in number_pages]
    debug_log(urls=str(urls))
    [results.extend(get_album_imgs(u[0])) for u in urls]
    return results

#---------------------------config------------------------------#

def parser_conf(fn="conf"):
    """ 解析配置文件 """
    conf = ini_file()
    conf.read(fn)
    try:
        save_path = conf.get("conf", "save_path")
    except NoOptionError:
        import os
        save_path = os.getcwd()

    try:
        db_path = conf.get("conf", "db_path")
    except NoOptionError:
        db_path = "img.db"

    try:
        size = conf.get("conf", "size").split(";")
    except NoOptionError:
        size = ["0x0"]

    try:
        urls = conf.get("conf", "urls").split(";")
    except NoOptionError:
        urls = ["http://www.topit.me/user/335565"]

    return {"save_path": save_path,
            "size": size,
            "urls": urls,
            "db_path": db_path}

def parser_db(fn):
    """ 读取数据库信息 """
    db = bsddb.btopen(fn)
    return db

def filter_size(func):
    def wrapper(**args):
        while(1):
            regex = """href=\"(.*)\"\sname=.*&gt;&lt;img\swidth=\"(\d+)\"\sheight=\"(\d+)\""""
            debug_log(args=str(args))
            img_url, width, height = re.findall(regex, args["page"])[0]

            if args.has_key("dst_size") == False:
                args = {"img_url": None}

            width = int(width)*1.25
            height = int(height)*1.25

            f_min = -args["floor"]
            f_max = args["floor"]

#            debug_log(dst_size=str(args["dst_size"]))

            for size in args["dst_size"]:
                w_gap = width-int(size[0])
                h_gap = height-int(size[1])

                if (w_gap &gt;= f_min and w_gap &lt;= f_max) and \
                   (h_gap &gt;= f_min and h_gap &lt;= f_max):
                   continue
                else:
                    args = {"img_url": None}
                    break
            args = {"width": width,
                    "height": height,
                    "img_url": img_url}
            break
        return func(**args)
    return wrapper

def filter_scale(func):
    def wrapper(**args):
        v = args["height"]/args["width"]
        if v &gt;=1.3 and v &lt;= 1.6:
            return func(**args)
        else:
            return func(img_url=None)
    return wrapper

@filter_size
@filter_scale
def get_img(**args):
    return args

def worker(tasks, db, save_path, dst_size):
    """ 判断大小,下载图片,写入数据库 """
    try:
        url, tid, url_path = tasks.pop()
    except IndexError:
        return

    tid = tid.encode("utf-8")
    if db.has_key(tid.encode("utf-8")) == True:
        return
    page = tidy_page(url)
    result = get_img(page=page, dst_size=dst_size, floor=20)
    if result["img_url"] == None:
        return
    img = down_page(result["img_url"])
    w = result["width"]
    h = result["height"]

    p = save_path+url_path
    try:
        os.listdir(p)
    except OSError:
        os.makedirs(p)
    file_path = "%s/%s%s" % (p, tid, ".jpg")
    open(file_path, "wb").write(img)
    v = "%s:%s:%s" % (url_path, w, h)
    debug_log(insert_db=v)
    db[tid] = v

#---------------------------start------------------------------#

def main():
    conf = parser_conf()
    db = parser_db(conf["db_path"])

    urls = []
    results = []

    #todo 加入判断数据库成员是否存在,如果不存在就干掉对应的key

    for url in conf["urls"]:
        if url.find("search") &gt;=0:
            results.extend(get_search_imgs(url))
        else:
            results.extend(get_album_imgs(url))
    # results = json.loads(open("blackberry.db", "r").read())

    while(len(results) &gt; 0):
        worker(results, db, conf["save_path"], conf["size"])

if __name__ == '__main__':
    main()
</code></pre> ]]>
        </content>
    </entry>
    <entry>
        <title type="html"><![CDATA[test ]]></title>
        <author><name>shiweifu</name><uri>http://shiweifu.github.com</uri></author>
        <link href="http://shiweifu.github.com/2012/test.html"/>
        <updated>2012-07-07T00:00:00Z</updated>
        <published>2012-07-07T00:00:00Z</published>
        <id>http://shiweifu.github.com/2012/test.html</id>
        <category scheme="http://shiweifu.github.com/tag/life/" term="life" label="Life" />
        <content type="html" xml:base="http://shiweifu.github.com" xml:lang="en">
            <![CDATA[ <p>test</p> ]]>
        </content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用 Python 的动态特性]]></title>
        <author><name>shiweifu</name><uri>http://shiweifu.github.com</uri></author>
        <link href="http://shiweifu.github.com/2012/python的动态特性.html"/>
        <updated>2012-05-12T00:00:00Z</updated>
        <published>2012-05-12T00:00:00Z</published>
        <id>http://shiweifu.github.com/2012/python的动态特性.html</id>
        <category scheme="http://shiweifu.github.com/tag/python/" term="python" label="Python" />
        <content type="html" xml:base="http://shiweifu.github.com" xml:lang="en">
            <![CDATA[ <p>使用Python 的动态特性，可以优化掉很多冗余的代码。
编写oauth程序的时候，会遇到这种情况：</p>
<p><em>GET /blocks/ids    获取用户黑名单id列表      </em>
<em>GET /blocks/blocking   获取黑名单上用户资料  </em>
<em>POST /blocks/create    把指定id用户加入黑名单</em> <br />
<em>GET /blocks/exists 检查用户是否被加入了黑名单 </em><br />
<em>POST /blocks/destroy   将指定id用户解除黑名单</em></p>
<p>这些都是饭否开放的REST接口，用户通过调用这些接口，来实现对应的功能，然后我写出了类似这样的代码：</p>
<pre lang="Python" colla="-"> 
class UrlToken(object):
    """docstring for UrlToken"""
    url = ""
    method = -1
    must_has = "status"
    def __init__(self, _url, _method):
        super(UrlToken, self).__init__()
        self.url = _url
        self.method = _method

class FanfouStatusHandle(FanfouHandle):
    """docstring for FanfouUsersHandle"""        

    def __init__(self, _account):
        super(FanfouStatusHandle2, self).__init__()
        self.account = _account
        self.headers = self.account.get_headers()
        self.consumer_token  = self.account.get_consumer_token()

    def update(self, status):
        """ update user state"""
        tok = UrlToken("POST", "/status/update.json")
        _http_call(tok)

    def destroy(self, msg_id = None):
        """ destroy fanfou status by msg_id """
        tok = UrlToken("POST", "/status/destroy.json")
        _http_call(tok)

class FanfouAccountHandle(FanfouHandle):
    def __init__(self, _account):
        super(FanfouStatusHandle2, self).__init__()
        self.account = _account
        self.headers = self.account.get_headers()
        self.consumer_token  = self.account.get_consumer_token()

    def verify_credentials(self):
        """ verify_credentials """
        tok = UrlToken("GET", "/account/verify_credentials.json")
        _http_call(tok)
</pre>

<p>这么写，用户调用很方便，每个接口模块写成一个类，再写个lib类，把这些单独的类再都实现一遍，然后调用就成了：
lib.statuses.update(status="hello lib")
用起来还算舒服，但看着一坨一坨的重复代码，太ugly了。</p>
<p>因为之前写的大多是C语言的程序，所以对反射这种高级货不太了解，基本上对这些动态特性，停留在“知道”。
知道有hasattr,getattr这些用于自省的特性，但从来没用过，那就拿这个程序下手。</p>
<p>其实调用最终都是调到_http_call这里去，不同的是：
1、函数名
2、UrlToken</p>
<p>可以通过重载<strong>getattr</strong>来获得要调用的函数名，然后在里面写个wrapper，用闭包来做不同函数的调用。
<pre lang="Python" colla="-"> </p>
<p>class Handle:
    def <strong>init</strong>(self, _toks):
        #toks是个字典结构，里面以kv的形式存储了需要的UrlToken
        self.toks = _toks</p>
<pre><code>def __getattr__(self, attr):
    if self.toks.has_key(attr) == False:
        raise NotImplemetedError
    tok = self.toks[attr]

    def wrapper(**kw):
        return _http_call(tok,kw)
</code></pre>
</pre>

<p>这么写之后，只需要构建一个大的UrlToken集合，里面有不同的地址和调用方式再封装个FanfouLib的类就可以了。</p>
<p>具体的代码，请参看项目。
地址：https://github.com/shiweifu/fanfoulib</p> ]]>
        </content>
    </entry>
    </feed>